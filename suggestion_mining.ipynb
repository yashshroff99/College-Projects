{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashshroff99/College-Projects/blob/main/suggestion_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCPgVxwjTumt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0dbe490e-c301-4731-db96-782a71b48747"
      },
      "source": [
        "pip install Pillow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": false,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "Iwq2RLS1TgvM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "111b73af-d7ba-456d-c183-958f4d27a8ab"
      },
      "source": [
        "# import required packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, gc, time, warnings\n",
        "\n",
        "from scipy.misc import imread\n",
        "from scipy import sparse\n",
        "import scipy.stats as ss\n",
        "from scipy.sparse import csr_matrix, hstack, vstack\n",
        "\n",
        "import matplotlib.pyplot as plt, matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud ,STOPWORDS\n",
        "from PIL import Image\n",
        "import matplotlib_venn as venn\n",
        "import pydot, graphviz\n",
        "from IPython.display import Image\n",
        "\n",
        "import string, re, nltk, collections\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization\n",
        "from keras.layers import GRU, LSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_kg_hide-input": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "sEQrZOwxTgvg"
      },
      "source": [
        "# settings\n",
        "\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "start_time = time.time()\n",
        "color = sns.color_palette()\n",
        "sns.set_style(\"dark\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "eng_stopwords = set(stopwords.words(\"english\"))\n",
        "lem = WordNetLemmatizer()\n",
        "ps = PorterStemmer()\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "413525163c3d8d0870200e1da91596e8578f7860",
        "id": "bKmZJZrqTgvx"
      },
      "source": [
        "# import the dataset\n",
        "\n",
        "train = pd.read_csv(\"/content/train\", encoding = 'latin-1')\n",
        "dev = pd.read_csv(\"/content/dev.csv\", encoding = 'latin-1')\n",
        "test = pd.read_csv(\"/content/test.csv\", encoding = 'latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": false,
        "_uuid": "14036485a842bbdc85443e4fe4d9bbcabb11960a",
        "id": "aKuYo7I1TgwB"
      },
      "source": [
        "# quick look at a few training examples\n",
        "\n",
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": false,
        "_uuid": "e1e6686c29472e0142885d37b12a122c50c1f615",
        "id": "pKVBalXhTgwJ"
      },
      "source": [
        "print(\"Training data...\")\n",
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5c942b44b014dac379c6b51735a22328af7e74e8",
        "id": "mKgqpEhITgwR"
      },
      "source": [
        "# class-imbalance in training data\n",
        "\n",
        "suggestion_count = (train['label'].values == 1).astype(int).sum()\n",
        "non_suggestion_count = (train['label'].values == 0).astype(int).sum()\n",
        "print(\"Total sentences : \" + str(train.shape[0]))\n",
        "print(\"Total suggestions : \" + str(suggestion_count))\n",
        "print(\"Total non_suggestions : \" + str(non_suggestion_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9a494095fbd0f6e4b9b7fbbf176d9721707d5040",
        "id": "qtjRueDdTgwb"
      },
      "source": [
        "# oversampling to balance the training data\n",
        "\n",
        "suggestions = train[train['label'].values == 1]\n",
        "\n",
        "while suggestion_count < non_suggestion_count:\n",
        "    random_suggestion = suggestions.sample()\n",
        "    train = train.append(random_suggestion, ignore_index = True)\n",
        "    suggestion_count = suggestion_count + 1\n",
        "\n",
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": false,
        "_uuid": "5afc1b93676d3c90eaff1672d864f932a2cb2afe",
        "id": "wpEQOkXfTgwj"
      },
      "source": [
        "# exploring the development/validation data\n",
        "\n",
        "print(\"Development Set...\")\n",
        "dev.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5513e4881d7047721dd24d4b9984b646f9c67daf",
        "id": "MetAcQxlTgwq"
      },
      "source": [
        "# class-imbalance in development data\n",
        "\n",
        "suggestion_count = (dev['label'].values == 1).astype(int).sum()\n",
        "non_suggestion_count = (dev['label'].values == 0).astype(int).sum()\n",
        "print(\"Total sentences : \" + str(dev.shape[0]))\n",
        "print(\"Total suggestions : \" + str(suggestion_count))\n",
        "print(\"Total non_suggestions : \" + str(non_suggestion_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "1db0439989c4f2ce53a54c7ec96960afc0929b01",
        "id": "oD1K5ZPlTgw1"
      },
      "source": [
        "stopword = set(STOPWORDS)\n",
        "\n",
        "# wordcloud for sentences with 'suggestion' label\n",
        "subset = train[train.label == 1]\n",
        "content = subset.sentence.values\n",
        "wc = WordCloud(background_color = \"black\", max_words = 2000, stopwords = stopword)\n",
        "wc.generate(\" \".join(content))\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.subplot(221)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Words frequented in 'suggestion' sentences\", fontsize = 20)\n",
        "plt.imshow(wc.recolor(colormap = 'viridis', random_state = 17), alpha = 0.98)\n",
        "\n",
        "# wordcloud for sentences with 'non-suggestion' label\n",
        "subset = train[train.label == 0]\n",
        "content = subset.sentence.values\n",
        "wc = WordCloud(background_color = \"black\", max_words = 2000, stopwords = stopword)\n",
        "wc.generate(\" \".join(content))\n",
        "plt.subplot(222)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Words frequented in 'non-suggestion' sentences\", fontsize = 20)\n",
        "plt.imshow(wc.recolor(colormap = 'viridis', random_state = 17), alpha = 0.98)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "3587f7a8647810b5673a9498570e80ffcd8521d2",
        "id": "SZDDFej_TgxA"
      },
      "source": [
        "# Aphost lookup dict\n",
        "\n",
        "APPO = {\n",
        "    \"aren't\" : \"are not\",\n",
        "    \"can't\" : \"cannot\",\n",
        "    \"couldn't\" : \"could not\",\n",
        "    \"didn't\" : \"did not\",\n",
        "    \"doesn't\" : \"does not\",\n",
        "    \"don't\" : \"do not\",\n",
        "    \"hadn't\" : \"had not\",\n",
        "    \"hasn't\" : \"has not\",\n",
        "    \"haven't\" : \"have not\",\n",
        "    \"he'd\" : \"he would\",\n",
        "    \"he'll\" : \"he will\",\n",
        "    \"he's\" : \"he is\",\n",
        "    \"i'd\" : \"I would\",\n",
        "    \"i'd\" : \"I had\",\n",
        "    \"i'll\" : \"I will\",\n",
        "    \"i'm\" : \"I am\",\n",
        "    \"isn't\" : \"is not\",\n",
        "    \"it's\" : \"it is\",\n",
        "    \"it'll\":\"it will\",\n",
        "    \"i've\" : \"I have\",\n",
        "    \"let's\" : \"let us\",\n",
        "    \"mightn't\" : \"might not\",\n",
        "    \"mustn't\" : \"must not\",\n",
        "    \"shan't\" : \"shall not\",\n",
        "    \"she'd\" : \"she would\",\n",
        "    \"she'll\" : \"she will\",\n",
        "    \"she's\" : \"she is\",\n",
        "    \"shouldn't\" : \"should not\",\n",
        "    \"that's\" : \"that is\",\n",
        "    \"there's\" : \"there is\",\n",
        "    \"they'd\" : \"they would\",\n",
        "    \"they'll\" : \"they will\",\n",
        "    \"they're\" : \"they are\",\n",
        "    \"they've\" : \"they have\",\n",
        "    \"we'd\" : \"we would\",\n",
        "    \"we're\" : \"we are\",\n",
        "    \"weren't\" : \"were not\",\n",
        "    \"we've\" : \"we have\",\n",
        "    \"what'll\" : \"what will\",\n",
        "    \"what're\" : \"what are\",\n",
        "    \"what's\" : \"what is\",\n",
        "    \"what've\" : \"what have\",\n",
        "    \"where's\" : \"where is\",\n",
        "    \"who'd\" : \"who would\",\n",
        "    \"who'll\" : \"who will\",\n",
        "    \"who're\" : \"who are\",\n",
        "    \"who's\" : \"who is\",\n",
        "    \"who've\" : \"who have\",\n",
        "    \"won't\" : \"will not\",\n",
        "    \"wouldn't\" : \"would not\",\n",
        "    \"you'd\" : \"you would\",\n",
        "    \"you'll\" : \"you will\",\n",
        "    \"you're\" : \"you are\",\n",
        "    \"you've\" : \"you have\",\n",
        "    \"'re\": \" are\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'll\":\" will\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"tryin'\":\"trying\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "f1b9d5bf4b3adb978a3c1f91c3ae5d1e6b4fc48c",
        "id": "8YbEMRuPTgxI"
      },
      "source": [
        "def clean(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('<.*>', '', sentence)\n",
        "    sentence = re.sub(\"\\\\n\", \"\", sentence)\n",
        "    sentence = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"\", sentence)\n",
        "    sentence = re.sub(\"\\[\\[.*\\]\", \"\", sentence)\n",
        "    sentence = re.sub(\"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\", \"\", sentence)\n",
        "\n",
        "    words = tokenizer.tokenize(sentence)\n",
        "\n",
        "    words = [APPO[word] if word in APPO else word for word in words]\n",
        "    words = [ps.stem(word) for word in words]\n",
        "    words = [lem.lemmatize(word, \"v\") for word in words]\n",
        "    words = [w for w in words if not w in eng_stopwords]\n",
        "\n",
        "    clean_sent = \" \".join(words)\n",
        "\n",
        "    return(clean_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8cfdf908dc00c55671c03a31cdc5157626131f0f",
        "id": "IkfuIBWmTgxP"
      },
      "source": [
        "# obtaining separate clean corpora for suggestion and non-suggestion classes\n",
        "\n",
        "suggestion_corpus = train[train['label'].values == 1].sentence\n",
        "suggestion_corpus = suggestion_corpus.append(dev[dev['label'].values == 1].sentence)\n",
        "clean_suggestion_corpus = \"\"\n",
        "for sentence in suggestion_corpus:\n",
        "    clean_suggestion_corpus += clean(sentence)\n",
        "\n",
        "non_suggestion_corpus = train[train['label'].values == 0].sentence\n",
        "non_suggestion_corpus = non_suggestion_corpus.append(dev[dev['label'].values == 0].sentence)\n",
        "clean_non_suggestion_corpus = \"\"\n",
        "for sentence in non_suggestion_corpus:\n",
        "    clean_non_suggestion_corpus += clean(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "bf44113d90119657b9638cb058af73e56d92884d",
        "id": "RArD8KGeTgxV"
      },
      "source": [
        "# top 20 bigrams in suggestion sentences\n",
        "\n",
        "suggestion_bigrams = ngrams(clean_suggestion_corpus.split(), 2)\n",
        "suggestion_bigrams_freq = collections.Counter(suggestion_bigrams)\n",
        "suggestion_bigrams_freq.most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0fd64cdb36f80caa27efefd99dcf15bab550db81",
        "id": "oMjP-ip0Tgxf"
      },
      "source": [
        "# top 20 bigrams in non-suggestion sentences\n",
        "\n",
        "non_suggestion_bigrams = ngrams(clean_non_suggestion_corpus.split(), 2)\n",
        "non_suggestion_bigrams_freq = collections.Counter(non_suggestion_bigrams)\n",
        "non_suggestion_bigrams_freq.most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "86c0d4e9c88251fbe79e233cf98f55fae29e8570",
        "id": "iYnOqQ1QTgxn"
      },
      "source": [
        "del(suggestions)\n",
        "del(subset)\n",
        "del(content)\n",
        "del(stopword)\n",
        "del(wc)\n",
        "del(suggestion_corpus)\n",
        "del(clean_suggestion_corpus)\n",
        "del(non_suggestion_corpus)\n",
        "del(clean_non_suggestion_corpus)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "c80ddf190c5aa410b025d7914a316340af34d151",
        "id": "HtqXiu_ITgxy"
      },
      "source": [
        "# plot of sentence length against label\n",
        "\n",
        "df = pd.concat([train, dev])\n",
        "df['count_word'] = df['sentence'].apply(lambda x : len(x.split()))\n",
        "\n",
        "plt.figure(figsize = (12, 6))\n",
        "plt.suptitle(\"How is sentence length related to its label?\", fontsize = 15)\n",
        "count_word = df['count_word'].astype(int)\n",
        "df['count_word'].loc[df['count_word'] > 100] = 100\n",
        "plt.plot()\n",
        "sns.violinplot(y = 'count_word', x = 'label', data = df, split = True, inner = \"quart\")\n",
        "plt.xlabel('Suggestion?', fontsize = 12)\n",
        "plt.ylabel('Number of words in a sentence', fontsize = 12)\n",
        "plt.title(\"Number of sentences with a given word length\", fontsize = 12)\n",
        "plt.show()\n",
        "\n",
        "del(df)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "1ab4648b8ef4d02d048a280eb4012c561f42fb9b",
        "id": "zwEwlePXTgx8"
      },
      "source": [
        "# plot of mean word length against label\n",
        "\n",
        "df = pd.concat([train, dev])\n",
        "df['mean_word_len'] = df['sentence'].apply(lambda x : np.mean([len(word) for word in x.split()]))\n",
        "\n",
        "plt.figure(figsize = (12, 6))\n",
        "plt.suptitle(\"How is mean word length in a sentence related to its label?\", fontsize = 15)\n",
        "mean_word_len = df['mean_word_len'].astype(int)\n",
        "df['mean_word_len'].loc[df['mean_word_len'] > 10] = 10\n",
        "plt.plot()\n",
        "sns.violinplot(y = 'mean_word_len', x = 'label', data = df, split = True, inner = \"quart\")\n",
        "plt.xlabel('Suggestion?', fontsize = 12)\n",
        "plt.ylabel('Mean word length in sentence', fontsize = 12)\n",
        "plt.title(\"Number of sentences with a given mean word length\", fontsize = 12)\n",
        "plt.show()\n",
        "\n",
        "del(df)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cb411bd69aa55d40512ad4aff9c85e64cdf1036c",
        "id": "ofwlVbFDTgyN"
      },
      "source": [
        "# corpus containing all the sentences in train, development and test data\n",
        "\n",
        "corpus = (pd.concat([train.iloc[:, 0:2], dev.iloc[:, 0:2], test.iloc[:, 0:2]])).sentence\n",
        "clean_corpus = corpus.apply(lambda x : clean(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a00e0320c8c27eb91e8fcd160c4f97d7a286a9f4",
        "id": "PcZ1zQFOTgyW"
      },
      "source": [
        "# tf-idf vectors with unigram features\n",
        "\n",
        "unigram_tfv = TfidfVectorizer(strip_accents = 'unicode', analyzer = 'word', ngram_range = (1,1),\n",
        "                              sublinear_tf = 1, stop_words = 'english')\n",
        "unigram_tfv.fit(clean_corpus)\n",
        "\n",
        "train_unigrams = unigram_tfv.transform(clean_corpus.iloc[:train.shape[0]])\n",
        "dev_unigrams = unigram_tfv.transform(clean_corpus.iloc[train.shape[0]:train.shape[0]+dev.shape[0]])\n",
        "test_unigrams = unigram_tfv.transform(clean_corpus.iloc[train.shape[0]+dev.shape[0]:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "72a05a6d63f95ace49dbf2aa21825984a1e2ce77",
        "id": "cB4I1s4-Tgym"
      },
      "source": [
        "# tf-idf vectors with bigram and trigram features\n",
        "\n",
        "btgram_tfv = TfidfVectorizer(strip_accents = 'unicode', analyzer = 'word', ngram_range = (2,3),\n",
        "            sublinear_tf = 1, stop_words = 'english')\n",
        "btgram_tfv.fit(clean_corpus)\n",
        "\n",
        "train_btgrams = btgram_tfv.transform(clean_corpus.iloc[:train.shape[0]])\n",
        "dev_btgrams = btgram_tfv.transform(clean_corpus.iloc[train.shape[0]:train.shape[0]+dev.shape[0]])\n",
        "test_btgrams = btgram_tfv.transform(clean_corpus.iloc[train.shape[0]+dev.shape[0]:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f4d3b7ced16f1280ea7afdf8b554cefe8f1dc715",
        "id": "jXpbkTp3Tgyy"
      },
      "source": [
        "# tf-idf vectors with char n-gram features\n",
        "\n",
        "charngram_tfv = TfidfVectorizer(strip_accents = 'unicode', analyzer = 'char', ngram_range = (1,5),\n",
        "                sublinear_tf = 1, stop_words = 'english')\n",
        "charngram_tfv.fit(clean_corpus)\n",
        "\n",
        "train_charngrams =  charngram_tfv.transform(clean_corpus.iloc[:train.shape[0]])\n",
        "dev_charngrams = charngram_tfv.transform(clean_corpus.iloc[train.shape[0]:train.shape[0]+dev.shape[0]])\n",
        "test_charngrams = charngram_tfv.transform(clean_corpus.iloc[train.shape[0]+dev.shape[0]:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voZUz_NYTgy4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "ff99047f9c7eb21140c61d8b165ab49d83f779ae",
        "id": "pZV08Cj5Tgy_"
      },
      "source": [
        "# evaluation functions for different models\n",
        "\n",
        "def lgb_f1_score(preds, train_data):\n",
        "    y_train = train_data.get_label()\n",
        "    preds = (preds >= 0.5).astype(int)\n",
        "    return 'f1_score', f1_score(y_train, preds), True\n",
        "\n",
        "def xgb_f1_score(preds, train_data):\n",
        "    y_train = train_data.get_label()\n",
        "    preds = (preds >= 0.5).astype(int)\n",
        "    return 'f1_score', f1_score(y_train, preds)\n",
        "\n",
        "def nn_f1_score(y_true, y_pred):\n",
        "    y_pred = tf.cast((y_pred >= 0.5), tf.float32)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis = 0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis = 0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis = 0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis = 0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1_score = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmVRHovrTgzF"
      },
      "source": [
        "# dataframes for blending\n",
        "\n",
        "train_labels = pd.DataFrame()\n",
        "dev_labels = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "fae85a6e31a97cf83e79591fa4baac3eab8f0a43",
        "id": "j69ihmx0TgzK"
      },
      "source": [
        "# preparing data for statistical and GBDT models\n",
        "\n",
        "x_train = hstack((train_unigrams, train_btgrams, train_charngrams)).tocsr()\n",
        "y_train = train['label'].values\n",
        "x_dev = hstack((dev_unigrams, dev_btgrams, dev_charngrams)).tocsr()\n",
        "y_dev = dev['label'].values\n",
        "x_test = hstack((test_unigrams, test_btgrams, test_charngrams)).tocsr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b5f6a7b24ef3a739607a7503c1e3e487e283bd7d",
        "id": "GoQuy-x9TgzO"
      },
      "source": [
        "# logistic regression classifier\n",
        "\n",
        "clf = LogisticRegression(C = 0.1, solver = 'liblinear')\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "lr_dev_pred = clf.predict_proba(x_dev)[:, 1]\n",
        "lr_test_pred = clf.predict_proba(x_test)[:, 1]\n",
        "\n",
        "train_labels['lr'] = (clf.predict_proba(x_train)[:, 1] >= 0.5).astype(int)\n",
        "dev_labels['lr'] = (clf.predict_proba(x_dev)[:, 1] >= 0.5).astype(int)\n",
        "\n",
        "y_pred = (lr_dev_pred >= 0.5).astype(int)\n",
        "lr_precision = precision_score(y_dev, y_pred)\n",
        "lr_recall = recall_score(y_dev, y_pred)\n",
        "lr_f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(\"Logistic Regression...\")\n",
        "print(\"Precision score : \" + str(lr_precision))\n",
        "print(\"Recall score : \" + str(lr_recall))\n",
        "print(\"F1 score : \" + str(lr_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "01d4a4e807951ecb90fcebe8e70ea811a63fa1b5",
        "id": "DhXCCm8kTgzV"
      },
      "source": [
        "# SVM classifier\n",
        "\n",
        "# reducing the number of features using Singular Value Decomposition\n",
        "svd = TruncatedSVD(n_components = 15)\n",
        "svd.fit(vstack((x_train, x_dev, x_test)).tocsr())\n",
        "x_train_svd = svd.transform(x_train)\n",
        "x_dev_svd = svd.transform(x_dev)\n",
        "x_test_svd = svd.transform(x_test)\n",
        "\n",
        "# scaling the data obtained from SVD\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(np.concatenate((x_train_svd, x_dev_svd, x_test_svd)))\n",
        "x_train_svd = scaler.transform(x_train_svd)\n",
        "x_dev_svd = scaler.transform(x_dev_svd)\n",
        "x_test_svd = scaler.transform(x_test_svd)\n",
        "\n",
        "clf = SVC(C = 0.1, probability = True)\n",
        "clf.fit(x_train_svd, y_train)\n",
        "\n",
        "svm_dev_pred = clf.predict_proba(x_dev_svd)[:, 1]\n",
        "svm_test_pred = clf.predict_proba(x_test_svd)[:, 1]\n",
        "\n",
        "train_labels['svm'] = (clf.predict_proba(x_train_svd)[:, 1] >= 0.5).astype(int)\n",
        "dev_labels['svm'] = (clf.predict_proba(x_dev_svd)[:, 1] >= 0.5).astype(int)\n",
        "\n",
        "y_pred = (svm_dev_pred >= 0.5).astype(int)\n",
        "svm_precision = precision_score(y_dev, y_pred)\n",
        "svm_recall = recall_score(y_dev, y_pred)\n",
        "svm_f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(\"SVM Classifier...\")\n",
        "print(\"Precision score : \" + str(svm_precision))\n",
        "print(\"Recall score : \" + str(svm_recall))\n",
        "print(\"F1 score : \" + str(svm_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f307de5bdb325b5e5da388761005b4a25244943e",
        "id": "ZP06XVdeTgza"
      },
      "source": [
        "# lgbm classifier\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "d_train = lgb.Dataset(x_train, label = y_train)\n",
        "d_dev = lgb.Dataset(x_dev, label = y_dev)\n",
        "valid_sets = [d_train, d_dev]\n",
        "\n",
        "params = {'learning_rate': 0.2,\n",
        "          'application': 'binary',\n",
        "          'num_leaves': 31,\n",
        "          'verbosity': -1,\n",
        "          'bagging_fraction': 0.8,\n",
        "          'feature_fraction': 0.6,\n",
        "          'nthread': 4,\n",
        "          'lambda_l1': 1,\n",
        "          'lambda_l2': 1}\n",
        "\n",
        "model = lgb.train(params,\n",
        "                  train_set = d_train,\n",
        "                  num_boost_round = 25,\n",
        "                  valid_sets = valid_sets,\n",
        "                  feval = lgb_f1_score,\n",
        "                  verbose_eval = False)\n",
        "\n",
        "lgb_dev_pred = model.predict(x_dev)\n",
        "lgb_test_pred = model.predict(x_test)\n",
        "\n",
        "train_labels['lgb'] = (model.predict(x_train) >= 0.5).astype(int)\n",
        "dev_labels['lgb'] = (model.predict(x_dev) >= 0.5).astype(int)\n",
        "\n",
        "y_pred = (lgb_dev_pred >= 0.5).astype(int)\n",
        "lgb_precision = precision_score(y_dev, y_pred)\n",
        "lgb_recall = recall_score(y_dev, y_pred)\n",
        "lgb_f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(\"LGBM Classifier...\")\n",
        "print(\"Precision score : \" + str(lgb_precision))\n",
        "print(\"Recall score : \" + str(lgb_recall))\n",
        "print(\"F1 score : \" + str(lgb_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "dd0d2051da25af11681afc7f40683a993d55b9b7",
        "id": "Nm02BaEnTgzt"
      },
      "source": [
        "del(d_train)\n",
        "del(d_dev)\n",
        "del(model)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6eab63774a8f8f0a688f153b3e3acf7c42854c88",
        "id": "fcm-nO_8Tgzy"
      },
      "source": [
        "# XGBoost classifier\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "d_train = xgb.DMatrix(x_train, label = y_train)\n",
        "d_dev = xgb.DMatrix(x_dev, label = y_dev)\n",
        "d_test = xgb.DMatrix(x_test)\n",
        "evallist = [(d_train, 'train'), (d_dev, 'valid')]\n",
        "\n",
        "params = {'booster' : 'gbtree',\n",
        "          'nthread' : 4,\n",
        "          'eta' : 0.2,\n",
        "          'max_depth' : 6,\n",
        "          'min_child_weight' : 4,\n",
        "          'subsample' : 0.7,\n",
        "          'colsample_bytree' : 0.7,\n",
        "          'objective' : 'binary:logistic'}\n",
        "\n",
        "model = xgb.train(params,\n",
        "                  d_train,\n",
        "                  num_boost_round = 21,\n",
        "                  evals = evallist,\n",
        "                  feval = xgb_f1_score,\n",
        "                  verbose_eval = False)\n",
        "\n",
        "xgb_dev_pred = model.predict(d_dev, ntree_limit = 21)\n",
        "xgb_test_pred = model.predict(d_test, ntree_limit = 21)\n",
        "\n",
        "train_labels['xgb'] = (model.predict(d_train, ntree_limit = 21) >= 0.5).astype(int)\n",
        "dev_labels['xgb'] = (model.predict(d_dev, ntree_limit = 21) >= 0.5).astype(int)\n",
        "\n",
        "y_pred = (xgb_dev_pred >= 0.5).astype(int)\n",
        "xgb_precision = precision_score(y_dev, y_pred)\n",
        "xgb_recall = recall_score(y_dev, y_pred)\n",
        "xgb_f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(\"XGBoost Classifier...\")\n",
        "print(\"Precision score : \" + str(xgb_precision))\n",
        "print(\"Recall score : \" + str(xgb_recall))\n",
        "print(\"F1 score : \" + str(xgb_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "1c06b4abad89bd722b693a74db5a672fdd393d77",
        "id": "FqKFss26Tgz1",
        "outputId": "3cf780df-23a4-4d8d-9afe-348d6a40c141"
      },
      "source": [
        "del(x_train)\n",
        "del(y_train)\n",
        "del(x_dev)\n",
        "del(y_dev)\n",
        "del(d_train)\n",
        "del(d_dev)\n",
        "del(model)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "87135751e668ef92b5a9c1e9bf90db3e58948908",
        "id": "VrNBwfH8Tgz6"
      },
      "source": [
        "# preparing data for Neural Network\n",
        "\n",
        "EMBEDDING_FILE = '../input/fasttext/crawl-300d-2M.vec'\n",
        "\n",
        "max_features = 10760\n",
        "maxlen = 600\n",
        "embed_size = 300\n",
        "\n",
        "pos_tags_train = train['sentence'].apply(lambda x : \" \".join(item[1] for item in pos_tag(word_tokenize(x)))).values\n",
        "pos_tags_dev = dev['sentence'].apply(lambda x : \" \".join(item[1] for item in pos_tag(word_tokenize(x)))).values\n",
        "pos_tags_test = test['sentence'].apply(lambda x : \" \".join(item[1] for item in pos_tag(word_tokenize(x)))).values\n",
        "\n",
        "x_train = train['sentence'].values + \" \" + pos_tags_train\n",
        "y_train = train['label'].values\n",
        "x_dev = dev['sentence'].values + \" \" + pos_tags_dev\n",
        "y_dev = dev['label'].values\n",
        "x_test = test['sentence'].values + \" \" + pos_tags_test\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words = max_features)\n",
        "tokenizer.fit_on_texts(list(x_train) + list(x_dev) + list(x_test))\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_dev = tokenizer.texts_to_sequences(x_dev)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
        "x_dev = sequence.pad_sequences(x_dev, maxlen = maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen = maxlen)\n",
        "\n",
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype = 'float32')\n",
        "\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "5c08f2975aaf6cd5df7b892d8d40df92e8c25560",
        "id": "EGI1nzGoTgz8",
        "outputId": "ccdd5ac9-5465-4789-9109-a786911a8070"
      },
      "source": [
        "# Hybrid Neural Network classifier\n",
        "\n",
        "inp = Input(shape = (maxlen, ))\n",
        "x = Embedding(max_features, embed_size, weights = [embedding_matrix])(inp)\n",
        "x = SpatialDropout1D(0.2)(x)\n",
        "x = Bidirectional(GRU(100, return_sequences = True))(x)\n",
        "x = Conv1D(50, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
        "avg_pool = GlobalAveragePooling1D()(x)\n",
        "max_pool = GlobalMaxPooling1D()(x)\n",
        "conc = concatenate([avg_pool, max_pool])\n",
        "outp = Dense(1, activation = \"sigmoid\")(conc)\n",
        "\n",
        "model = Model(inputs = inp, outputs = outp)\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [nn_f1_score])\n",
        "model.fit(x_train, y_train, batch_size = 128, epochs = 1, validation_data = (x_dev, y_dev), verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 12830 samples, validate on 592 samples\n",
            "Epoch 1/1\n",
            "12830/12830 [==============================] - 169s 13ms/step - loss: 0.4102 - nn_f1_score: 0.8084 - val_loss: 0.4603 - val_nn_f1_score: 0.7897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f807b1780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "95468df192416a05faf9a31dcaa40f20f06f65e0",
        "id": "tu8xg4vvTg0A",
        "outputId": "d10d0958-17f1-46fa-c234-34143cfcd0be"
      },
      "source": [
        "nn_dev_pred = model.predict(x_dev, batch_size = 128, verbose = 1)\n",
        "nn_test_pred = model.predict(x_test, batch_size = 128, verbose = 1)\n",
        "\n",
        "train_labels['nn'] = (model.predict(x_train, batch_size = 128, verbose = 1) >= 0.5).astype(int)\n",
        "dev_labels['nn'] = (model.predict(x_dev, batch_size = 128, verbose = 1) >= 0.5).astype(int)\n",
        "\n",
        "y_pred = (nn_dev_pred >= 0.5).astype(int)\n",
        "nn_precision = precision_score(y_dev, y_pred)\n",
        "nn_recall = recall_score(y_dev, y_pred)\n",
        "nn_f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(\"Hybrid Neural Network Classifier...\")\n",
        "print(\"Precision score : \" + str(nn_precision))\n",
        "print(\"Recall score : \" + str(nn_recall))\n",
        "print(\"F1 score : \" + str(nn_f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "592/592 [==============================] - 3s 6ms/step\n",
            "833/833 [==============================] - 5s 5ms/step\n",
            "12830/12830 [==============================] - 64s 5ms/step\n",
            "592/592 [==============================] - 4s 6ms/step\n",
            "Hybrid Neural Network Classifier...\n",
            "Precision score : 0.8194945848375451\n",
            "Recall score : 0.7668918918918919\n",
            "F1 score : 0.7923211169284468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "3d2b548726105b7a8dc03ca58662bbfa59d8d914",
        "id": "Zy6tbIf3Tg0K"
      },
      "source": [
        "plot_model(model, to_file = 'model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht1Gkqk-Tg0V"
      },
      "source": [
        "def getmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 5, activation = 'relu'))\n",
        "    model.add(Dense(64, activation = 'relu'))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [nn_f1_score])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4fd115bc41a971ca3a59ee12e1befa705fb99737",
        "id": "WAsOlXkGTg0a"
      },
      "source": [
        "# Stacking all the models\n",
        "\n",
        "stacked_model = getmodel()\n",
        "\n",
        "stacked_model.fit(train_labels, y_train, batch_size = 128, epochs = 2, validation_data = (dev_labels, y_dev),\n",
        "          verbose = 1)\n",
        "\n",
        "stacked_dev_pred = stacked_model.predict(dev_labels, batch_size = 128, verbose = 1)\n",
        "\n",
        "y_pred = (stacked_dev_pred >= 0.5).astype(int)\n",
        "stack_precision = precision_score(y_dev, y_pred)\n",
        "stack_recall = recall_score(y_dev, y_pred)\n",
        "stack_f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(\"Stacked Models Classifier...\")\n",
        "print(\"Precision score : \" + str(stack_precision))\n",
        "print(\"Recall score : \" + str(stack_recall))\n",
        "print(\"F1 score : \" + str(stack_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f58ee5535464d14c3904e07f6fba01e907df9788",
        "id": "PcYaRu2kTg0q"
      },
      "source": [
        "# saving the test labels to output csv file\n",
        "\n",
        "y_test = (nn_test_pred[:, 0] >= 0.5).astype(int)\n",
        "submission = pd.read_csv(\"/content/test.csv\")\n",
        "submission.drop(['label'], axis = 1)\n",
        "submission['label'] = y_test\n",
        "submission.to_csv(\"test.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}